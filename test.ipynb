{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS 4300 - Spring 2025\n",
      "Practical 02\n",
      "Due:\n",
      "- Project needs to be functional for Exam on March 24.\n",
      "- Final deliverable and repo to be submitted by March 26 @ 11:59pm.\n",
      "Overview:\n",
      "In this project, you and your team will build a local Retrieval-Augmented Generation\n",
      "system that allows a user to query the collective DS4300 notes from members of your\n",
      "team. Your system will do the following:\n",
      "1. Ingest a collection of documents that represent material, such as course notes,\n",
      "you and your team have collected throughout the semester.\n",
      "2. Index those documents using embedding and a vector database\n",
      "3. Accept a query from the user.\n",
      "4. Retrieve relevant context based on the user’s query\n",
      "5. Package the relevant context up into a prompt that is passed to a locally-running\n",
      "LLM to generate a response.\n",
      "In this, you’ll experiment with different variables - chunking strategies, embedding models,\n",
      "some prompt engineering, local LLM, and vector database options. You’ll analyze how\n",
      "these changes affect the system’s performance and output quality.\n",
      "Teams and Corpus:\n",
      "Each team should be composed of 2 - 4 members. They can be the same as Practical\n",
      "01, but teams may switch up.\n",
      "Each team should gather a collection of course notes taken by the team members. This\n",
      "can be the slide decks, personal notes taken throughout, and additional documentation\n",
      "for the tools/systems we have used.\n",
      "Tools:\n",
      "- Python for building the pipeline\n",
      "- Ollama for running LLMs locally (you’ll compare and contrast at least 2 different\n",
      "models\n",
      "- Vector Databases (Redis Vector DB, Chroma, and one other of your choosing)\n",
      "- Embedding Models (you’ll compare and contrast at least 3 different options)\n",
      "Variables to Explore:\n",
      "1. Text preprocessing & chunking\n",
      "a. Try various size chunks: 200, 500, 1000 tokens, for example\n",
      "b. Try different chunk overlap sizes: 0, 50, 100 token overlap for example\n",
      "c. Try various basic text prep strategies such as removing whitespace,\n",
      "punctuation, and any other “noise”.\n",
      "2. Embedding Models - Choose 3 to compare and contrast. Examples include:\n",
      "a. sentence-transformers/all-MiniLM-L6-v2\n",
      "b. sentence-transformers/all-mpnet-base-v2\n",
      "c. InstructorXL\n",
      "d. The model we used in class\n",
      "Measure interesting properties of using the various embedding models such as\n",
      "speed, memory usage, and retrieval quality (qualitative).\n",
      "3. Vector Database - At a minimum, compare and contrast Redis Vector DB and\n",
      "Chroma (you’ll have to do a little research on this db) and one other vector\n",
      "database you choose based on research. Examine the speed of indexing and\n",
      "querying as well as the memory usage.\n",
      "4. Tweaks to the System prompt. Use the one from the class example as a starting\n",
      "point.\n",
      "5. Try at least 2 different local LLMs. Examples include Llama 2 7B and Mistral 7B.\n",
      "You aren’t required to use these two specifically, however.\n",
      "Suggested Steps:\n",
      "1. Collect and clean the data.\n",
      "a. If you’re using PDFs, review the output of whichever Python PDF library\n",
      "you’re using. Is it what you expect?\n",
      "b. Do you want to pre-process the raw text in some way before indexing?\n",
      "Perhaps remove extra white space or remove stop words?\n",
      "2. Implement a driver Python script to execute your various versions of the indexing\n",
      "pipeline and to collect important data about the process (memory, time, etc).\n",
      "Systematically vary the chunking strategies, embedding models, various prompt\n",
      "tweaks, choice of Vector DB, and choice of LLM.\n",
      "3. Develop a set of user questions that you give to each pipeline and qualitatively\n",
      "review the responses.\n",
      "4. Choose which pipeline you think works the best, and justify your choice.\n",
      "Deliverables:\n",
      "As a team, you’ll produce a slide deck communicating your findings as well as your final\n",
      "choice of pipelines with justification. Be specific. (Template for deck will be forthcoming.)\n",
      "You will also include a public GitHub repository containing well-organized set of scripts\n",
      "related to the various pipelines your team tests. The README should describe how to\n",
      "execute your project.\n",
      "More details on deliverables will be shared soon.\n",
      "Areas for Evaluation:\n",
      "1. Robustness of Experimentation (30%)\n",
      "2. Analysis of collected data (30%)\n",
      "3. Recommendation of pipeline organizations (20%)\n",
      "4. Professionalism of Slide Deck (20%)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"Practical 01 (1).pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open(\"data/Practical 01 (1).pdf\")\n",
    "text_by_page = []\n",
    "for page_num, page in enumerate(doc):\n",
    "    text_by_page.append((page_num, page.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'DS 4300 - Spring 2025  \\nPractical 02 \\n \\nDue:  \\n-\\u200b\\nProject needs to be functional for Exam on March 24. \\n-\\u200b\\nFinal deliverable and repo to be submitted by March 26 @ 11:59pm.  \\n \\nOverview: \\n \\nIn this project, you and your team will build a local Retrieval-Augmented Generation \\nsystem that allows a user to query the collective DS4300 notes from members of your \\nteam.  Your system will do the following: \\n \\n1.\\u200b Ingest a collection of documents that represent material, such as course notes, \\nyou and your team have collected throughout the semester.  \\n2.\\u200b Index those documents using embedding and a vector database \\n3.\\u200b Accept a query from the user.  \\n4.\\u200b Retrieve relevant context based on the user’s query \\n5.\\u200b Package the relevant context up into a prompt that is passed to a locally-running \\nLLM to generate a response.  \\n \\nIn this, you’ll experiment with different variables - chunking strategies, embedding models, \\nsome prompt engineering, local LLM, and vector database options.  You’ll analyze how \\nthese changes affect the system’s performance and output quality.  \\n \\nTeams and Corpus: \\n \\nEach team should be composed of 2 - 4 members.  They can be the same as Practical \\n01, but teams may switch up.  \\n \\nEach team should gather a collection of course notes taken by the team members.  This \\ncan be the slide decks, personal notes taken throughout, and additional documentation \\nfor the tools/systems we have used.   \\n \\nTools: \\n \\n-\\u200b\\nPython for building the pipeline \\n-\\u200b\\nOllama for running LLMs locally (you’ll compare and contrast at least 2 different \\n'), (1, 'models \\n-\\u200b\\nVector Databases (Redis Vector DB, Chroma, and one other of your choosing) \\n-\\u200b\\nEmbedding Models (you’ll compare and contrast at least 3 different options)   \\n \\nVariables to Explore: \\n \\n1.\\u200b Text preprocessing & chunking  \\na.\\u200b Try various size chunks: 200, 500, 1000 tokens, for example \\nb.\\u200b Try different chunk overlap sizes: 0, 50, 100 token overlap for example \\nc.\\u200b Try various basic text prep strategies such as removing whitespace, \\npunctuation, and any other “noise”.  \\n2.\\u200b Embedding Models - Choose 3 to compare and contrast. Examples include:  \\na.\\u200b sentence-transformers/all-MiniLM-L6-v2 \\nb.\\u200b sentence-transformers/all-mpnet-base-v2 \\nc.\\u200b InstructorXL \\nd.\\u200b The model we used in class \\nMeasure interesting properties of using the various embedding models such as \\nspeed, memory usage, and retrieval quality (qualitative).  \\n3.\\u200b Vector Database - At a minimum, compare and contrast Redis Vector DB and \\nChroma (you’ll have to do a little research on this db) and one other vector \\ndatabase you choose based on research.  Examine the speed of indexing and \\nquerying as well as the memory usage.  \\n4.\\u200b Tweaks to the System prompt. Use the one from the class example as a starting \\npoint.  \\n5.\\u200b Try at least 2 different local LLMs.  Examples include Llama 2 7B and Mistral 7B.  \\nYou aren’t required to use these two specifically, however.  \\n \\nSuggested Steps: \\n \\n1.\\u200b Collect and clean the data.   \\na.\\u200b If you’re using PDFs, review the output of whichever Python PDF library \\nyou’re using.  Is it what you expect?  \\nb.\\u200b Do you want to pre-process the raw text in some way before indexing?  \\nPerhaps remove extra white space or remove stop words? \\n2.\\u200b Implement a driver Python script to execute your various versions of the indexing \\npipeline and to collect important data about the process (memory, time, etc).  \\nSystematically vary the chunking strategies, embedding models, various prompt \\ntweaks, choice of Vector DB, and choice of LLM.  \\n3.\\u200b Develop a set of user questions that you give to each pipeline and qualitatively \\n'), (2, 'review the responses.  \\n4.\\u200b Choose which pipeline you think works the best, and justify your choice.  \\n \\nDeliverables: \\n \\nAs a team, you’ll produce a slide deck communicating your findings as well as your final \\nchoice of pipelines with justification. Be specific.  (Template for deck will be forthcoming.) \\n \\nYou will also include a public GitHub repository containing well-organized set of scripts \\nrelated to the various pipelines your team tests.  The README should describe how to \\nexecute your project.   \\n \\nMore details on deliverables will be shared soon.  \\n \\n \\nAreas for Evaluation: \\n1.\\u200b Robustness of Experimentation (30%) \\n2.\\u200b Analysis of collected data (30%) \\n3.\\u200b Recommendation of pipeline organizations (20%) \\n4.\\u200b Professionalism of Slide Deck (20%) \\n')]\n"
     ]
    }
   ],
   "source": [
    "print(text_by_page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
